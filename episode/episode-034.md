# Episode
- 第34回（2022/05/16）: AIの公平性の課題である「Model Fairness」について考える
    - リンク: https://anchor.fm/double-m2/episodes/34-Model-Fairness-e1iit9v

# Agenda
- Model Fairness（モデルの公平性）について2人で議論しました

# Contents
- **Model Fairnessとは？**
  - モデルの品質や状態が公平・公正であるか
  - 企業に対して以下に挙げた損害を与える可能性がある
    - 長期的な損失
    - 顧客離れ
    - 評判の低下
    - 透明性の低下
- 学習データに社会的なバイアスが含まれているとAIシステムはアンフェアな振る舞いをするかもしれない
- 例：
> アマゾンの採用アルゴリズムの事例。このアルゴリズムは、過去10年間に同社が受け取ったカリキュラムを学習させ、他の履歴書より目立つものを抽出することを狙ったのです。技術職においては、採用アルゴリズムが「女性はこの職種には向いていない」と判断していたのです。技術職は歴史的に男性が担ってきた職種であるため、女性の候補者に関する情報が相対的に少なかったことが要因として挙げられます。技術職の候補者を性別に関係なく採点していないことが発覚しなければ、このような問題は起きなかったでしょう。
- **Harm of allocation**
  - AIシステムは部分集団に関係したリソース・情報・機会に影響を与えることがある
    - 例：学校の入試・採用・融資など，特定の集団の中から候補者を選ぶ方が，他の集団の中から選ぶよりもモデルの性能が高い場合
      - モデルは特定のグループの人々の中から他のグループよりも優れた候補者を選ぶのにはるかに優れている可能性があります
- **Harm of quality-of-service**
  - AIシステムがあるグループに対して他のグループよりも良いサービスを提供することで，2つの特定のグループに対して同等のパフォーマンスを発揮することができない場合
    - 例：音声認識システムは男性へのサービスと同様に，女性にも満足のいくサービスを提供できないかもしれない
- **偏りを避けるためのベストプラクティス**
  - 公平性分析を個別の取り組みとして行うのではなく，MLプロセス全体を通して組み合わせること
  - 公平性の観点からモデルを再評価することは，データセットの不均衡の発見・予測閾値の設定・本番環境のモデル動作の理解・予測分析の実行などのステップを含む
- **公平性が求められるということ**
  - AIの説明責任が求められているからこそ、公平性に対する問題も盛り上がってきたのかも
- **公平性を評価するPython Package**
  - **Fairlearn**
    - Demographic Parity
      - データ全体でデモグラフィック属性が均等になっているか
    - Equal Opportunity
      - モデルの出力結果が差別を助長する可能性がある要因（性別など）の影響を受けていないか

# Reference
- [What is Model Fairness - Model Monitoring | MLOps Wiki](https://censius.ai/wiki/model-fairness)
- [Fairness analysis for AL models](https://medium.com/@arnautiendat/fairness-analysis-for-ai-algorithms-d4e069c9d3f1)
- [Fairlearn - Python Package](https://fairlearn.org/)
